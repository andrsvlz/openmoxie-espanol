#version: "3.9"
services:
  data-init:
    build:
      context: .
      dockerfile: web.Dockerfile         # reuse the web image so Django is available
    env_file: .env
    environment:
      DJANGO_SETTINGS_MODULE: openmoxie.settings
    command: >
      bash -lc "
        cd /app/site &&
        python manage.py migrate --noinput &&
        python manage.py init_data || echo 'init_data skipped (possibly already seeded)'
      "
    volumes:
      - ./site/work:/app/site/work       # same DB/logs volume as web
      - ./site/static:/app/site/static   # optional, harmless
    restart: "no"
  web:
    build:
      context: .
      dockerfile: web.Dockerfile
    env_file: .env
    environment:
      DJANGO_SETTINGS_MODULE: openmoxie.settings
      STT_URL: http://stt:8001/stt
      OLLAMA_HOST: http://ollama:11434
      LLM_PROVIDER: ollama
      OLLAMA_MODEL: llama3.2:3b
      STT_BACKEND: local
      STT_LANG: en
      MQTT_HOST: mqtt
      MQTT_PORT: "8883"
      WEB_WORKERS: "1"
    ports:
      - "8000:8000"
    volumes:
      - ./site/work:/app/site/work
      - ./site/static:/app/site/static
    #depends_on:
    #  - mqtt
    #  - stt
    #  - ollama

    depends_on:
          model-init:
            condition: service_completed_successfully   # if you use model-init
          data-init:
            condition: service_completed_successfully
          stt:
            condition: service_started
          mqtt:
            condition: service_started
          ollama:
            condition: service_started      
    restart: unless-stopped

  # seeds /models with Faster-Whisper models, then exits
  model-init:
    image: curlimages/curl:8.10.1
    volumes:
      - ./docker/seed-models.sh:/seed-models.sh:ro
      - ./site/services/stt/models:/models
    environment:
      MODEL_NAMES: "faster-whisper-small.en,faster-whisper-base.en"  # comma-separated
      MODELS_DIR: "/models"
    entrypoint: ["/bin/sh","-c","/seed-models.sh"]
    restart: "no"

  stt:
    build:
      context: .
      dockerfile: stt.Dockerfile
    ports:
      - "8001:8001"
    #volumes:
    #  - ./site/services/stt/models:/models:ro
    environment:
      STT_MODEL: /models/faster-whisper-small.en
      STT_DEVICE: "auto"
      STT_COMPUTE: "int8"
      STT_VAD: "1"
    restart: unless-stopped
    # GPU (optional):
    # gpus: all
    depends_on:
      model-init:
        condition: service_completed_successfully
    volumes:
      - ./site/services/stt/models:/models:ro   # stt reads the seeded models

  mqtt:
    build:
      context: .
      dockerfile: mqtt.Dockerfile
    ports:
      - "8883:8883"
    volumes:
      - ./keys:/keys:ro
      - ./local/work:/work
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    #ports:
      #- "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    # GPU (optional):
    # gpus: all

volumes:
  ollama:
