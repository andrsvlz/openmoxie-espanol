
services:

  mqtt:
    build:
      context: ./
      dockerfile: ./mqtt.Dockerfile
      platforms:
          - linux/amd64
          - linux/arm64
    image: openmoxie/openmoxie-mqtt:${OPENMOXIE_VERSION:-latest}
    ports:
      - "8883:8883"
    volumes:
      - ./local/work:/mosquitto/log

  server:
    build:
      context: ./
      dockerfile: ./Dockerfile
      platforms:
          - linux/amd64
          - linux/arm64
    image: openmoxie/openmoxie-server:${OPENMOXIE_VERSION:-latest}
    ports:
      - "8001:8000"
    volumes:
      - ./local/work:/app/site/work
    depends_on:
      - mqtt
      
  stt:
    build:
      context: .
      dockerfile: stt.Dockerfile
    ports:
      - "8001:8001"
    # Mount your local models so itâ€™s fully offline
    volumes:
      - ./site/services/stt/models:/models:ro
    environment:
      # Pick one of the model dirs you already have:
      #   /models/faster-whisper-small.en  or  /models/faster-whisper-base.en
      STT_MODEL: /models/faster-whisper-small.en
      STT_DEVICE: "auto"          # "auto" | "cpu" | "cuda"
      STT_COMPUTE: "int8"         # CPU best: int8 ; GPU best: float16
      STT_VAD: "1"                # silence filtering
    restart: unless-stopped

    # === Enable GPU (optional) ===
    # Uncomment ONE of the following blocks depending on your Docker/Compose setup.

    # Newer Compose / Docker (works on most setups):
    # gpus: all

    # Or the lower-level device_requests syntax:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]